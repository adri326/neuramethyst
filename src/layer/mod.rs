use crate::algebra::NeuraVectorSpace;

use self::lock::NeuraLockLayer;

pub mod dense;
pub mod dropout;
pub mod isolate;
pub mod lock;
pub mod normalize;
pub mod softmax;

#[derive(Clone, Copy, PartialEq, Debug)]
pub enum NeuraShape {
    Vector(usize),               // entries
    Matrix(usize, usize),        // rows, columns
    Tensor(usize, usize, usize), // rows, columns, channels
}

impl NeuraShape {
    pub fn size(&self) -> usize {
        match self {
            NeuraShape::Vector(entries) => *entries,
            NeuraShape::Matrix(rows, columns) => rows * columns,
            NeuraShape::Tensor(rows, columns, channels) => rows * columns * channels,
        }
    }

    pub fn sub(&self, other: NeuraShape) -> Option<NeuraShape> {
        use NeuraShape::*;

        Some(match (other, self) {
            (Vector(x1), Vector(x2)) => Vector(x2 - x1),
            (Matrix(x1, y1), Matrix(x2, y2)) => Matrix(x2 - x1, y2 - y1),
            (Tensor(x1, y1, z1), Tensor(x2, y2, z2)) => Tensor(x2 - x1, y2 - y1, z2 - z1),

            _ => return None,
        })
    }

    pub fn is_compatible(&self, other: NeuraShape) -> bool {
        use NeuraShape::*;

        matches!(
            (self, other),
            (Vector(_), Vector(_))
                | (Matrix(_, _), Matrix(_, _))
                | (Tensor(_, _, _), Tensor(_, _, _))
        )
    }

    pub fn dims(&self) -> usize {
        match self {
            NeuraShape::Vector(_) => 1,
            NeuraShape::Matrix(_, _) => 2,
            NeuraShape::Tensor(_, _, _) => 3,
        }
    }
}

impl From<usize> for NeuraShape {
    fn from(x: usize) -> Self {
        NeuraShape::Vector(x)
    }
}

impl From<(usize, usize)> for NeuraShape {
    fn from((x, y): (usize, usize)) -> Self {
        NeuraShape::Matrix(x, y)
    }
}

impl From<(usize, usize, usize)> for NeuraShape {
    fn from((x, y, z): (usize, usize, usize)) -> Self {
        NeuraShape::Tensor(x, y, z)
    }
}

pub trait NeuraLayerBase: std::fmt::Debug + Clone + 'static {
    /// What type the gradient of the layer is
    type Gradient: NeuraVectorSpace + Send + 'static;

    /// What the desired output shape of the layer is
    fn output_shape(&self) -> NeuraShape;

    /// Constructor for the gradient, should return the zero vector
    fn default_gradient(&self) -> Self::Gradient;

    /// Applies `δW_l` to the weights of the layer, the default implementation is a noop
    #[allow(unused_variables)]
    #[inline(always)]
    fn apply_gradient(&mut self, gradient: &Self::Gradient) {
        // Noop
    }

    /// Arbitrary computation that can be executed at the start of an epoch
    #[allow(unused_variables)]
    #[inline(always)]
    fn prepare_layer(&mut self, is_training: bool) {
        // Noop
    }

    /// Computes the regularization terms of the layer's gradient, called once per batch
    #[inline(always)]
    fn regularize_layer(&self) -> Self::Gradient {
        self.default_gradient()
    }

    fn lock_layer(self) -> NeuraLockLayer<Self>
    where
        Self: Sized,
    {
        NeuraLockLayer::new(self)
    }
}

pub trait NeuraLayer<Input>: NeuraLayerBase {
    /// What type the layer outputs, may depend on `Input`.
    type Output;

    /// A type that can hold data between calls to `eval_training`, `backprop_layer` and `get_gradient`
    type IntermediaryRepr: 'static;

    fn eval(&self, input: &Input) -> Self::Output {
        self.eval_training(input).0
    }

    fn eval_training(&self, input: &Input) -> (Self::Output, Self::IntermediaryRepr);

    /// Computes the layer's gradient,
    ///
    /// `intermediary` is guaranteed to have been generated by a previous call to `eval_training`,
    /// without mutation of `self` in-between, and with the same `input`.
    #[allow(unused_variables)]
    #[inline(always)]
    fn get_gradient(
        &self,
        input: &Input,
        intermediary: &Self::IntermediaryRepr,
        epsilon: &Self::Output,
    ) -> Self::Gradient {
        self.default_gradient()
    }

    /// Computes the backpropagation term and the derivative of the internal weights,
    /// using the `input` vector outputted by the previous layer and the backpropagation term `epsilon` of the next layer.
    ///
    /// Note: we introduce the term `epsilon`, which together with the activation of the current function can be used to compute `delta_l`:
    /// ```no_rust
    /// f_l'(a_l) * epsilon_l = delta_l
    /// ```
    ///
    /// The function should then return a pair `(epsilon_{l-1}, δW_l)`,
    /// with `epsilon_{l-1}` being multiplied by `f_{l-1}'(activation)` by the next layer to obtain `delta_{l-1}`.
    /// Using this intermediate value for `delta` allows us to isolate it computation to the respective layers.
    fn backprop_layer(
        &self,
        input: &Input,
        intermediary: &Self::IntermediaryRepr,
        epsilon: &Self::Output,
    ) -> Input;
}

#[deprecated]
pub trait NeuraShapedLayer: NeuraLayerBase {}

pub trait NeuraPartialLayer {
    type Constructed: NeuraLayerBase + 'static;
    type Err;

    fn construct(self, input_shape: NeuraShape) -> Result<Self::Constructed, Self::Err>;
}

#[deprecated]
pub trait NeuraTrainableLayerBase: NeuraLayerBase {}

#[deprecated]
pub trait NeuraTrainableLayerEval<Input>: NeuraLayer<Input> {}

/// Contains methods relative to a layer's ability to compute its own weights gradients,
/// given the derivative of the output variables.
#[deprecated]
pub trait NeuraTrainableLayerSelf<Input>: NeuraLayer<Input> {}

#[deprecated]
pub trait NeuraTrainableLayerBackprop<Input>: NeuraLayer<Input> {}

impl NeuraLayerBase for () {
    type Gradient = ();

    #[inline(always)]
    fn default_gradient(&self) -> Self::Gradient {
        
    }

    #[inline(always)]
    fn apply_gradient(&mut self, _gradient: &Self::Gradient) {
        // Noop
    }

    fn output_shape(&self) -> NeuraShape {
        panic!("() has no shape!");
    }
}

impl<Input: Clone> NeuraLayer<Input> for () {
    type Output = Input;
    type IntermediaryRepr = ();

    #[inline(always)]
    fn eval_training(&self, input: &Input) -> (Self::Output, Self::IntermediaryRepr) {
        (input.clone(), ())
    }

    #[inline(always)]
    fn backprop_layer(
        &self,
        _input: &Input,
        _intermediary: &Self::IntermediaryRepr,
        epsilon: &Self::Output,
    ) -> Input {
        epsilon.clone()
    }
}
/// Temporary implementation of neura_layer
#[macro_export]
macro_rules! neura_layer {
    ( "dense", $output:expr, $type:ty ) => {{
        let res: $crate::layer::dense::NeuraDenseLayerPartial<$type, _, _, _> =
            $crate::layer::dense::NeuraDenseLayer::new_partial(
                $output,
                rand::thread_rng(),
                $crate::derivable::activation::LeakyRelu(0.1),
                $crate::derivable::regularize::NeuraL0,
            );
        res
    }};
    ( "dense", $output:expr ) => {
        $crate::neura_layer!("dense", $output, f32)
    };

    ( "dropout", $probability:expr ) => {
        $crate::layer::dropout::NeuraDropoutLayer::new($probability, rand::thread_rng())
    };

    ( "softmax" ) => {
        $crate::layer::softmax::NeuraSoftmaxLayer::new()
    };

    ( "normalize" ) => {
        $crate::layer::normalize::NeuraNormalizeLayer::new()
    };

    ( "isolate", $start:expr, $end:expr ) => {
        $crate::layer::isolate::NeuraIsolateLayer::new($start, $end).unwrap()
    };
}
