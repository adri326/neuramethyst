use crate::algebra::NeuraVectorSpace;

use self::lock::NeuraLockLayer;

pub mod dense;
pub mod dropout;
pub mod lock;
pub mod normalize;
pub mod softmax;

#[derive(Clone, Copy, PartialEq, Debug)]
pub enum NeuraShape {
    Vector(usize),               // entries
    Matrix(usize, usize),        // rows, columns
    Tensor(usize, usize, usize), // rows, columns, channels
}

impl NeuraShape {
    pub fn size(&self) -> usize {
        match self {
            NeuraShape::Vector(entries) => *entries,
            NeuraShape::Matrix(rows, columns) => rows * columns,
            NeuraShape::Tensor(rows, columns, channels) => rows * columns * channels,
        }
    }
}

pub trait NeuraLayer<Input> {
    /// What type the layer outputs
    type Output;

    fn eval(&self, input: &Input) -> Self::Output;

    fn lock_layer(self) -> NeuraLockLayer<Self>
    where
        Self: Sized,
    {
        NeuraLockLayer::new(self)
    }
}

impl<Input: Clone> NeuraLayer<Input> for () {
    type Output = Input;

    #[inline(always)]
    fn eval(&self, input: &Input) -> Self::Output {
        input.clone()
    }
}

pub trait NeuraShapedLayer {
    fn output_shape(&self) -> NeuraShape;
}

pub trait NeuraPartialLayer {
    type Constructed: NeuraShapedLayer;
    type Err;

    fn construct(self, input_shape: NeuraShape) -> Result<Self::Constructed, Self::Err>;
}

pub trait NeuraTrainableLayerBase {
    /// The representation of the layer gradient as a vector space
    type Gradient: NeuraVectorSpace;

    fn default_gradient(&self) -> Self::Gradient;

    /// Applies `δW_l` to the weights of the layer
    fn apply_gradient(&mut self, gradient: &Self::Gradient);

    /// Arbitrary computation that can be executed at the start of an epoch
    #[allow(unused_variables)]
    #[inline(always)]
    fn prepare_layer(&mut self, is_training: bool) {}
}

pub trait NeuraTrainableLayerEval<Input>: NeuraTrainableLayerBase + NeuraLayer<Input> {
    /// An intermediary object type to be passed to the various training methods
    type IntermediaryRepr;

    // TODO: move this into another trait
    fn eval_training(&self, input: &Input) -> (Self::Output, Self::IntermediaryRepr);
}

/// Contains methods relative to a layer's ability to compute its own weights gradients,
/// given the derivative of the output variables.
pub trait NeuraTrainableLayerSelf<Input>: NeuraTrainableLayerEval<Input> {
    /// Computes the regularization
    fn regularize_layer(&self) -> Self::Gradient;

    /// Computes the layer's gradient,
    ///
    /// `intermediary` is guaranteed to have been generated by a previous call to `eval_training`,
    /// without mutation of `self` in-between, and with the same `input`.
    fn get_gradient(
        &self,
        input: &Input,
        intermediary: &Self::IntermediaryRepr,
        epsilon: &Self::Output,
    ) -> Self::Gradient;
}

// impl<Input, Layer: NeuraTrainableLayerBase<Input, Gradient = ()>> NeuraTrainableLayerSelf<Input>
//     for Layer
// {
//     #[inline(always)]
//     fn regularize_layer(&self) -> Self::Gradient {
//         ()
//     }

//     #[inline(always)]
//     fn get_gradient(
//         &self,
//         input: &Input,
//         intermediary: &Self::IntermediaryRepr,
//         epsilon: Self::Output,
//     ) -> Self::Gradient {
//         ()
//     }
// }

pub trait NeuraTrainableLayerBackprop<Input>: NeuraTrainableLayerEval<Input> {
    /// Computes the backpropagation term and the derivative of the internal weights,
    /// using the `input` vector outputted by the previous layer and the backpropagation term `epsilon` of the next layer.
    ///
    /// Note: we introduce the term `epsilon`, which together with the activation of the current function can be used to compute `delta_l`:
    /// ```no_rust
    /// f_l'(a_l) * epsilon_l = delta_l
    /// ```
    ///
    /// The function should then return a pair `(epsilon_{l-1}, δW_l)`,
    /// with `epsilon_{l-1}` being multiplied by `f_{l-1}'(activation)` by the next layer to obtain `delta_{l-1}`.
    /// Using this intermediate value for `delta` allows us to isolate it computation to the respective layers.
    fn backprop_layer(
        &self,
        input: &Input,
        intermediary: &Self::IntermediaryRepr,
        epsilon: &Self::Output,
    ) -> Input;
}

impl NeuraTrainableLayerBase for () {
    type Gradient = ();

    #[inline(always)]
    fn default_gradient(&self) -> Self::Gradient {
        ()
    }

    #[inline(always)]
    fn apply_gradient(&mut self, _gradient: &Self::Gradient) {
        // Noop
    }
}

impl<Input: Clone> NeuraTrainableLayerEval<Input> for () {
    type IntermediaryRepr = ();

    #[inline(always)]
    fn eval_training(&self, input: &Input) -> (Self::Output, Self::IntermediaryRepr) {
        (self.eval(input), ())
    }
}

/// Temporary implementation of neura_layer
#[macro_export]
macro_rules! neura_layer {
    ( "dense", $output:expr, $type:ty ) => {{
        let res: $crate::layer::dense::NeuraDenseLayerPartial<$type, _, _, _> =
            $crate::layer::dense::NeuraDenseLayer::new_partial(
                $output,
                rand::thread_rng(),
                $crate::derivable::activation::LeakyRelu(0.1),
                $crate::derivable::regularize::NeuraL0,
            );
        res
    }};
    ( "dense", $output:expr ) => {
        $crate::neura_layer!("dense", $output, f32)
    };

    ( "dropout", $probability:expr ) => {
        $crate::layer::dropout::NeuraDropoutLayer::new($probability, rand::thread_rng())
    };

    ( "softmax" ) => {
        $crate::layer::softmax::NeuraSoftmaxLayer::new()
    };

    ( "normalize" ) => {
        $crate::layer::normalize::NeuraNormalizeLayer::new()
    };
}
